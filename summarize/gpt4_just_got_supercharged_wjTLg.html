<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">GPT-4 Just Got Supercharged!</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<h2>ChatGPT Updates and User Customization</h2>

<p><strong>ChatGPT</strong> has been updated to provide <strong>more direct responses</strong> and <strong>less meandering answers</strong>, which marks a significant improvement. Users now have the ability to <strong>customize their ChatGPT experience</strong> by adding instructions like "Give me brief answers, don't be too formal and always cite your sources" for a tailored experience. This update has notably enhanced ChatGPT's capabilities in <strong>writing, math, logical reasoning, and coding</strong>, though the extent of improvement varies across these areas.</p>

<h2>Performance Improvements and Comparisons</h2>

<ul>

<li><strong>Reading comprehension</strong> and <strong>GPQA dataset performance</strong> have seen improvements, with the latter showing significant advancements.</li>

<li><strong>GPT-4's mathematical abilities</strong> have notably improved, achieving a 72% success rate on a dataset that challenges even international mathematical olympiad gold medalists.</li>

<li>In <strong>code generation</strong>, as measured by the HumanEval dataset, GPT-4's performance is slightly worse, indicating varied performance across different tasks.</li>

<li>Compared to GPT-4, <strong>Anthropicâ€™s Claude 3</strong> is noted to excel in reasoning tasks.</li>

</ul>

<h2>AI Advancements and Evaluations</h2>

<p>The progress in AI is likened to the evolution of <strong>self-driving cars</strong>, highlighting that advancements often come with both improvements and setbacks. The <strong>Chatbot Arena leaderboard</strong>, which assigns an Elo score based on user preferences, serves as a tool to evaluate chatbot performance. On this leaderboard, <strong>GPT-4 ranks first</strong>, with <strong>Claude 3 Opus</strong> and <strong>Command-R+</strong> from Cohere also performing well, indicating competitive advancements in AI chatbots. <strong>Claude 3 Haiku</strong> is highlighted for being significantly cheaper than GPT-4 while still being capable and maintaining long conversations.</p>

<h2>User Experience and Access</h2>

<p>To access the updated ChatGPT, users should check the <strong>knowledge cutoff date</strong> at chat.openai.com, with a more recent date indicating access to the updated version. There's a mention of <strong>Devin</strong>, an AI system designed to work like a software engineer, with a caution about potentially overstated capabilities based on a new credible source.</p>

<h2>Credibility and Research</h2>

<p>The importance of discussing <strong>peer-reviewed research papers</strong> is emphasized, along with the challenge of covering interesting, non-peer-reviewed topics without overstating results. There's a commitment to improving how potential pitfalls are highlighted when discussing non-peer-reviewed but interesting topics. The speaker shares anticipation for attending a conference, meeting fellow scholars, and presenting newly designed gifts, along with excitement about upcoming papers to be shared.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>