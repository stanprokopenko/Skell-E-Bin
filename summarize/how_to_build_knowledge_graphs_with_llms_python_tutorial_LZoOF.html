<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">How to Build Knowledge Graphs With LLMs (python tutorial)</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<p>In my previous video, I explored the fascinating interplay between graph databases and large language models. I demonstrated how to generate a knowledge graph from unstructured data and interact with it using a language model chat interface. This follow-up video delves into the code details, guiding you through the process from scratch.</p>

<h2>Setting Up the Environment</h2>

<h3>Data Types and Structure</h3>

<p>We're working with three types of documents: profiles of people, project briefs, and Slack messages. These documents include various details such as names, skills, project descriptions, and technologies used. All data, including JSON and markdown files, was generated by ChatGPT.</p>

<h3>Environment Variables and Python Libraries</h3>

<p>The project requires an <code>.env</code> file for storing credentials for the OpenAI API and the Neo4j database. The necessary Python libraries are listed in the requirements file.</p>

<h3>Setting Up Credentials</h3>

<p>For the OpenAI API, you'll need an API key, API base (endpoint), and API version. These are obtained from Azure or directly from OpenAI. For Neo4j, you'll need the endpoint URL, username, and password for database access. Neo4j's Aura service offers a convenient setup for this.</p>

<h2>Extracting Entities and Relationships</h2>

<h3>Function to Call OpenAI API</h3>

<p>A function is defined to interact with the OpenAI API, sending system messages or prompts and receiving responses. This function simplifies calling the API for various tasks.</p>

<h3>Extracting Entities and Relationships Function</h3>

<p>This function processes a folder of files, using a language model to identify entities and relationships within the data. It iterates over each file, applies a template prompt specific to the file type, and calls the OpenAI API to process the content. The results are compiled into a JSON object listing all identified entities and relationships.</p>

<h2>Generating and Executing Cypher Statements</h2>

<h3>Generating Cypher Statements Function</h3>

<p>This function takes the JSON output from the previous step and generates Cypher statements for creating entities and relationships in the Neo4j database. It meticulously constructs statements to ensure that nodes and relationships are correctly represented in the graph database, using the <code>merge</code> keyword to avoid duplicates.</p>

<h2>Running the Pipeline</h2>

<h3>Pipeline Function</h3>

<p>A comprehensive function orchestrates the entire process, from extracting entities and relationships to generating and executing Cypher statements. It processes all specified folders, applies the appropriate prompt templates, and ultimately creates the knowledge graph in Neo4j.</p>

<h2>Cost of Using GPT Models</h2>

<p>The cost of using GPT models for this project is discussed, with insights into monitoring and managing expenses through the Azure portal. The video highlights the affordability of processing a significant amount of data with GPT models.</p>

<p style="text-align: center;">* * *</p>

<p>This video provided a step-by-step guide to creating a knowledge graph from unstructured data using graph databases and large language models. The next part of this series will focus on building the chat interface to the graph database, offering a seamless way to interact with the generated knowledge graph.</p>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>