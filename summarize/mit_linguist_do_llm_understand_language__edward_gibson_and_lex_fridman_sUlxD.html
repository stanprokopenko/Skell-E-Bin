<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">MIT linguist: Do LLM understand language? | Edward Gibson and Lex Fridman</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<h2>Current Theories of Human Language</h2>

<ul>

<li><strong>Large language models (LLMs) are arguably the best current theories of human language</strong><ul>

<li>They excel at predicting what is valid in a language like English</li>

<li>They cover all the data - anything you want to say in the language</li>

</ul>

</li>

<li>However, LLMs as theories have some limitations:<ul>

<li>They are huge "black box" models with many parameters</li>

<li>They lack the simplicity and interpretability of a good theory</li>

</ul>

</li>

</ul>

<h2>What Makes a Good Language Theory</h2>

<ul>

<li>A good theory should be simple and able to explain the data</li>

<li>LLMs could potentially generate simpler explanations <ul>

<li>For example, they might be able to produce a dependency grammar for a language</li>

</ul>

</li>

<li>There is evidence LLMs may be implementing abstractions like dependency grammar internally</li>

</ul>

<h2>Construction-Based Theories of Language</h2>

<ul>

<li>Construction-based theories are based on form-meaning pairs for pieces of language</li>

<li>They are usage-based, dealing with the things people actually say and write</li>

<li>A construction is defined as a word or combination of words</li>

<li>These theories are not fully formalized - dependency grammar could potentially be a formalization</li>

</ul>

<h2>Do LLMs Understand Meaning or Just Mimic Form?</h2>

<ul>

<li>LLMs excel at modeling language form, but may not understand deeper meaning</li>

<li>They can be fooled by changes to problem formulations they've seen before<ul>

<li>An example is the Monty Hall problem - LLMs make errors humans wouldn't when the scenario is explained</li>

</ul>

</li>

<li>Humans are less likely to make certain errors LLMs make when the situation is clearly explained</li>

<li>LLMs show impressive ability to generate true statements, possibly because truth is over-represented in their training data compared to falsehood</li>

</ul>

<h2>Similarities and Differences to Human Language Processing</h2>

<ul>

<li>LLMs struggle with nested/center-embedded structures in a way very similar to humans<ul>

<li>This suggests they are modeling aspects of how humans process language</li>

</ul>

</li>

<li>There are differences in the ability to understand certain problems after explanation </li>

<li>LLMs have the potential to be "superhuman" on some specific language tasks</li>

</ul>

<h2>Potential Limits of the LLM Approach</h2>

<ul>

<li>There seem to be no apparent limits on LLMs' ability to model language form</li>

<li>However, they may lack aspects of human reasoning and understanding of meaning </li>

<li>LLMs may be missing some fundamental components of human language processing that occur in the brain</li>

<li>It's still an open question what the limits of LLMs are and how similar they truly are to human cognition</li>

</ul>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>