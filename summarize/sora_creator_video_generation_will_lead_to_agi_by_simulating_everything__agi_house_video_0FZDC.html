<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Skell-E HTML Preview</title>
  <link rel="stylesheet" href="../assets/style.css">
</head>
<body>
  <article>
    <header>
        <h1 id="title">Sora Creator “Video generation will lead to AGI by simulating everything” | AGI House Video</h1>
        <a href="#" class="button copy-link" onclick="copyToClipboard(); return false;">Copy Code</a>
    </header>
    <main id="code-to-copy" class="container">
<ul>

<li>Developed by Anthropic to generate high-quality, long-duration videos</li>

<li>Capable of generating 1080p videos up to a minute long</li>

<li>Learns about the physical world and object permanence from training on videos</li>

</ul>

<h3>Sora's Video Generation Capabilities</h3>

<ul>

<li>Can generate videos in various styles (e.g. papercraft world)</li>

<li>Understands and renders 3D space and geometry</li>

<li>Maintains object consistency across multiple shots</li>

</ul>

<h2>Revolutionizing Content Creation</h2>

<ul>

<li><strong>Potential for generating movie trailers and special effects</strong><ul>

<li>Persisting characters across shots</li>

<li>Creating fantastical effects more affordably than traditional CGI</li>

</ul>

</li>

<li><strong>Enabling creation of content that would be difficult to shoot traditionally</strong><ul>

<li>Example: Jewelry store with exotic animals in New York City</li>

</ul>

</li>

<li><strong>Democratizing content creation for people with creative ideas</strong></li>

</ul>

<h2>Working with Artists</h2>

<ul>

<li>Anthropic providing Sora access to a small pool of artists for feedback</li>

<li>Artists excited by Sora's potential for creating surreal and novel content<ul>

<li>Example: Shy Kids' video with balloon-headed character</li>

</ul>

</li>

<li>Gathering feedback to make Sora valuable and safe for wider use</li>

</ul>

<h2>Technology Behind Sora</h2>

<ul>

<li>Uses a scalable approach similar to language models<ul>

<li>Transforms videos into "patches" (analogous to tokens)</li>

<li>Trains Transformers on these patches to create a generalist model</li>

</ul>

</li>

<li>Ability to train on and generate videos of various aspect ratios</li>

<li>Zero-shot video editing capabilities using CLIP-guided diffusion</li>

<li>Can interpolate between two different input videos</li>

</ul>

<h2>Importance of Video Models for AGI</h2>

<ul>

<li>Video models like Sora are on the critical path to AGI<ul>

<li>Requires understanding of human interactions, physics, environments</li>

</ul>

</li>

<li>Sora's performance improves significantly with increased training compute<ul>

<li>Emergent capabilities expected as the model scales, similar to language models</li>

</ul>

</li>

<li>Potential for Sora to simulate not just the real world, but other environments like Minecraft</li>

</ul>

<h3>Current Limitations and Future Goals</h3>

<ul>

<li>Sora still struggles with certain physical interactions<ul>

<li>Shattering objects, complex collisions</li>

</ul>

</li>

<li>Considered the "GPT-1 of video" with significant room for improvement</li>

<li>Gathering feedback from artists on desired controls and capabilities</li>

<li>Focusing on responsible and safe deployment before providing wider access</li>

</ul>
    </main>
  </article>
  <footer><span class="skell-e-sig">&Sigma;&lt;3||-E</p></footer>
  <script src="../assets/script.js"></script>
</body>
</html>